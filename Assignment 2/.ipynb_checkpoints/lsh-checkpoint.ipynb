{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "About 100.000 users that watched in total 17.770 movies;\n",
    "- Each user watched between 300 and 3000 movies\n",
    "- The file contains about 65.000.000 records (720 MB) of the form:\n",
    "`<user_id, movie_id> : “user_id watched movie_id”`\n",
    "- Similarity between users: Jaccard similarity of sets of movies they watched: \n",
    "``` \n",
    "jsim(S1, S2) = #intersect(S1, S2)/#union(S1, S2)\n",
    "``` \n",
    "- Task: find (with help of LSH) pairs of users whose jsim > 0.5\n",
    "\n",
    "Process:\n",
    "1. Tune it (signature length, number of bands, number of rows per band)\n",
    "2. Randomize, optimize, benchmark, polish the code, ...\n",
    "3. Dump results to a text file ans.txt (just a csv list of records: user1, user2) \n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "t0 = time.time()\n",
    "np.random.seed(seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  load the data\n",
    "FILE = '../data/user_movie.npy'\n",
    "df = pd.DataFrame(np.load(FILE), columns = ['user','movie'])\n",
    "\n",
    "m_by_u = df.groupby('user')['movie'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHash\n",
    "\n",
    "The dataset is so small that random permutations is used instead of hash functions. \n",
    "In this way we can avoid time consuming loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set user as the column, movie as the row in the sparse matrix\n",
    "mat = csc_matrix(([1]*df.shape[0], (df.iloc[:,1], df.iloc[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_movie = mat.shape[0]\n",
    "n_user = mat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relatively short signatures (50-150) should result in good results (and take less time to compute).\n",
    "n_sig = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140  signatures remaining to be created\n",
      "130  signatures remaining to be created\n",
      "120  signatures remaining to be created\n",
      "110  signatures remaining to be created\n",
      "100  signatures remaining to be created\n",
      "90  signatures remaining to be created\n",
      "80  signatures remaining to be created\n",
      "70  signatures remaining to be created\n",
      "60  signatures remaining to be created\n",
      "50  signatures remaining to be created\n",
      "40  signatures remaining to be created\n",
      "30  signatures remaining to be created\n",
      "20  signatures remaining to be created\n",
      "10  signatures remaining to be created\n",
      "0  signatures remaining to be created\n"
     ]
    }
   ],
   "source": [
    "def minhashing(mat, n_sig = n_sig):\n",
    "    # create the sig matrix using minhashing\n",
    "    sig_matrix = np.zeros([n_sig, n_user])\n",
    "    for i in range(n_sig):\n",
    "        perm = mat[np.random.permutation(n_movie)]\n",
    "        for j in range(n_user):\n",
    "            sig_matrix[i,j] = int(perm.indices[perm.indptr[j]:perm.indptr[j+1]].min())\n",
    "        if i%10==9: # check the progress\n",
    "            print(n_sig - i - 1,' signatures remaining to be created')\n",
    "    return sig_matrix\n",
    "sig_matrix = minhashing(mat,n_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From data importing to minhashing, it takes 6.28 minutes\n"
     ]
    }
   ],
   "source": [
    "cal_time = round(time.time()-t0)\n",
    "print(\"From data importing to minhashing, it takes {0:.2f} minutes\".format(cal_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 103703)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dimension of the sig matrix\n",
    "sig_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.savetxt('sig.txt', sig_matrix, fmt='%d')\n",
    "#sig_matrix = np.loadtxt('sig.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the LSH algorithm\n",
    "\n",
    "When there are many users that fall into the same bucket (i.e., there are many candidates for being similar to each other) then checking if all the potential pairs are really similar might be very expensive: you have to check k(k-1)/2 pairs, when the bucket has k elements. Postpone evaluation of such a bucket till the very end (or just ignore it – they are really expensive). Or better: consider increasing the number of rows per band – that will reduce the chance of encountering big buckets.\n",
    "\n",
    "Note that b*r doesn’t have to be exactly the length of the signature. For example, when you work with signature of length n=100, you may consider e.g., b=3, r=33, b=6, r=15, etc. – any combination of b\n",
    " \n",
    "To make sure that your program will not exceed the 30 minutes runtime you are advised to close the result.txt file after any new pair is appended to it (and open it again, when you want to append a new one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_estimate_pair(pair,n_sig):\n",
    "    # check if the pair from the bucket is similar based on sig matrix\n",
    "    sigi = sig_matrix[:,pair[0]]#.tolist()#.ravel().tolist()[0]\n",
    "    sigj = sig_matrix[:,pair[1]]#.tolist()#.ravel().tolist()[0]\n",
    "    # estimate_jsim\n",
    "    intersect = np.count_nonzero(sigi == sigj)\n",
    "    jsim = intersect/n_sig\n",
    "    if jsim > 0.40:\n",
    "        return True\n",
    "\n",
    "def check_true_pair(pair):\n",
    "    # check if the similar pair based on sig matrix is a true pair\n",
    "    a = m_by_u[pair[0]]\n",
    "    b = m_by_u[pair[1]]\n",
    "    intersect = len(set(a) & set(b))\n",
    "    union = len(a) + len(b) - intersect\n",
    "    jsim = intersect/union\n",
    "    if jsim > 0.5:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many slices would give a lot of false positives while too few slices would only be able to identify the highest degrees of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bands = 25\n",
    "rows = 6\n",
    "max_bucket_item = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note\n",
    "bands = 30\n",
    "rows = 5\n",
    "max_bucket_item = 300\n",
    "Unique pairs:  21482770\n",
    "Similar pairs based on sig matrix:  878065\n",
    "True similar pairs:  639\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check, no write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSH_bucket(sig_matrix, max_bucket_item = 25):\n",
    "    unique_pairs = set() \n",
    "    for b in range(bands):\n",
    "        bucket_list = defaultdict(set) # remove the duplicate items\n",
    "        s = np.sum(sig_matrix[b*rows:(b+1)*rows,:], axis =0)\n",
    "        for index, x in np.ndenumerate(s):\n",
    "            bucket_list[x].add(index[0])\n",
    "        for key, value in bucket_list.items():\n",
    "            if len(value) > 1 and len(value) < max_bucket_item: \n",
    "                for i in value:\n",
    "                    for j in value:\n",
    "                        if i < j and ((i,j) not in unique_pairs): # remove the duplicate pair\n",
    "                            unique_pairs.add((i,j))\n",
    "    return unique_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSH_bucket_2(sig_matrix, max_bucket_item = 25):\n",
    "    unique_pairs = set() \n",
    "    for b in range(bands):\n",
    "        bucket_list = defaultdict(set) # remove the duplicate items\n",
    "        df = pd.DataFrame(sig_matrix[b*rows:(b+1)*rows,:])\n",
    "        h = df.apply(lambda x: hash(tuple(x)), axis = 0)\n",
    "        for index, x in np.ndenumerate(h):\n",
    "            bucket_list[x].add(index[0])\n",
    "        for key, value in bucket_list.items():\n",
    "            if len(value) > 1 and len(value) < max_bucket_item: \n",
    "                for i in value:\n",
    "                    for j in value:\n",
    "                        if i < j and ((i,j) not in unique_pairs): # remove the duplicate pair\n",
    "                            unique_pairs.add((i,j))\n",
    "    return unique_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_not_write():\n",
    "    unique_pairs = LSH_bucket_2(sig_matrix,max_bucket_item)\n",
    "    print('Unique pairs: ',len(unique_pairs))\n",
    "\n",
    "    sim_pairs = []\n",
    "    for pair in unique_pairs:\n",
    "        if check_estimate_pair(pair,n_sig):\n",
    "            sim_pairs.append(pair)\n",
    "    print('Similar pairs based on sig matrix: ',len(sim_pairs))\n",
    "\n",
    "    true_pairs = []\n",
    "    for pair in sim_pairs:\n",
    "        if check_true_pair(pair):\n",
    "            true_pairs.append(pair) \n",
    "    print('True similar pairs: ', len(true_pairs))\n",
    "#print_not_write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to text file, for final submital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSH_bucket_true_paris(sig_matrix, max_bucket_item = 25): \n",
    "    unique_pairs = set() \n",
    "    for b in range(bands):\n",
    "        bucket_list = defaultdict(set) # remove the duplicate items\n",
    "        df = pd.DataFrame(sig_matrix[b*rows:(b+1)*rows,:])\n",
    "        h = df.apply(lambda x: hash(tuple(x)), axis = 0)\n",
    "        for index, x in np.ndenumerate(h):\n",
    "            bucket_list[x].add(index[0])\n",
    "        for key, value in bucket_list.items():\n",
    "            if len(value) > 1 and len(value) < max_bucket_item: \n",
    "                for i in value:\n",
    "                    for j in value:\n",
    "                        if i < j and ((i,j) not in unique_pairs): # remove the duplicate pair\n",
    "                            unique_pairs.add((i,j))\n",
    "                            if check_estimate_pair((i,j),n_sig):\n",
    "                                if check_true_pair((i,j)):\n",
    "                                    with open('ans.txt','a') as file:\n",
    "                                        writer = csv.DictWriter(file,fieldnames=['user1', 'user2'])\n",
    "                                        writer.writerow({'user1':i,'user2':j})\n",
    "LSH_bucket_true_paris(sig_matrix,max_bucket_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time is 13.45 minutes\n"
     ]
    }
   ],
   "source": [
    "cal_time = round(time.time()-t0)\n",
    "print(\"Total running time is {0:.2f} minutes\".format(cal_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- [hash tuple](https://stackoverflow.com/questions/25757042/create-hash-value-for-each-row-of-data-with-selected-columns-in-dataframe-in-pyt/25757564)\n",
    "- [hash string](https://www.pythoncentral.io/hashing-strings-with-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
