{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "About 100.000 users that watched in total 17.770 movies;\n",
    "- Each user watched between 300 and 3000 movies\n",
    "- The file contains about 65.000.000 records (720 MB) of the form:\n",
    "`<user_id, movie_id> : “user_id watched movie_id”`\n",
    "- Similarity between users: Jaccard similarity of sets of movies they watched: \n",
    "``` \n",
    "jsim(S1, S2) = #intersect(S1, S2)/#union(S1, S2)\n",
    "``` \n",
    "- Task: find (with help of LSH) pairs of users whose jsim > 0.5\n",
    "\n",
    "Process:\n",
    "1. Tune it (signature length, number of bands, number of rows per band)\n",
    "2. Randomize, optimize, benchmark, polish the code, ...\n",
    "3. Dump results to a text file ans.txt (just a csv list of records: user1, user2) \n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "t0 = time.time()\n",
    "np.random.seed(seed=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  load the data\n",
    "FILE = '../data/user_movie.npy'\n",
    "df = pd.DataFrame(np.load(FILE), columns = ['user','movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique movies 17770\n",
      "movie starts at 0\n",
      "movieID ends at 17769\n"
     ]
    }
   ],
   "source": [
    "# data exploration\n",
    "m_by_u = df.groupby('user')['movie'].apply(list)\n",
    "n_user = len(df.user.unique())\n",
    "n_movie = len(df.movie.unique())\n",
    "print(\"Number of unique movies\", n_movie)\n",
    "print(\"movie starts at\", df.movie.min())\n",
    "print(\"movieID ends at\", df.movie.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the min and max movieID, the movieID attribute is continous without gap.\n",
    "\n",
    "## MinHash\n",
    "\n",
    "The dataset is so small that random permutations is used instead of hash functions. \n",
    "In this way we can avoid time consuming loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set user as the column, movie as the row in the sparse matrix\n",
    "mat = csc_matrix(([1]*df.shape[0], (df.iloc[:,1], df.iloc[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relatively short signatures (50-150) should result in good results (and take less time to compute).\n",
    "n_sig = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minhashing(mat, n_sig = n_sig):\n",
    "    sig_matrix = np.array([])\n",
    "    for i in range(n_sig):\n",
    "        seq = np.random.permutation(n_movie)\n",
    "        perm = mat[seq]\n",
    "        sig = perm.argmax(axis=0) #seq[perm.argmax(axis=0)]\n",
    "        if sig_matrix.any():\n",
    "            # return the index of the first occurrence of 1\n",
    "            sig_matrix = np.vstack((sig_matrix,sig))\n",
    "        else:\n",
    "            sig_matrix = sig\n",
    "        if i%10=0:\n",
    "            print(i,' signatures have been created\\n'))\n",
    "    return sig_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_matrix = minhashing(mat,n_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total minhashing time is 11.82 minutes\n"
     ]
    }
   ],
   "source": [
    "sig_matrix.shape\n",
    "cal_time = round(time.time()-t0)\n",
    "print(\"From importing data to minhashing, it takes{0:.2f} minutes\".format(cal_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the LSH algorithm\n",
    "\n",
    "When there are many users that fall into the same bucket (i.e., there are many candidates for being similar to each other) then checking if all the potential pairs are really similar might be very expensive: you have to check k(k-1)/2 pairs, when the bucket has k elements. Postpone evaluation of such a bucket till the very end (or just ignore it – they are really expensive). Or better: consider increasing the number of rows per band – that will reduce the chance of encountering big buckets.\n",
    "\n",
    "Note that b*r doesn’t have to be exactly the length of the signature. For example, when you work with signature of length n=100, you may consider e.g., b=3, r=33, b=6, r=15, etc. – any combination of b\n",
    " \n",
    "To make sure that your program will not exceed the 30 minutes runtime you are advised to close the result.txt file after any new pair is appended to it (and open it again, when you want to append a new one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_jsim(a,b):\n",
    "    intersect = set(a) & set(b)\n",
    "    union = set(a) | set(b)\n",
    "    jsim = len(intersect)/len(union) # intersect = A+B - union\n",
    "    return jsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSH_bucket(sig_matrix, max_bucket_item = 25): \n",
    "    if bands * rows != sig_matrix.shape[0]:\n",
    "        print('#bands * #rows is not equal to #signatures')\n",
    "    unique_pairs = set() \n",
    "    for b in range(bands):\n",
    "        bucket_list = defaultdict(set) # remove the duplicate items\n",
    "        s = np.sum(sig_matrix[b*rows:(b+1)*rows,:], axis =0)\n",
    "        for index, x in np.ndenumerate(s):\n",
    "            bucket_list[x].add(index[1])\n",
    "            #print(index,x)\n",
    "        #print(bucket_list.items())\n",
    "        for key, value in bucket_list.items():\n",
    "            if len(value) > 1 and len(value) < max_bucket_item: \n",
    "                for i in value:\n",
    "                    for j in value:\n",
    "                        if i < j and ((i,j) not in unique_pairs): # remove the duplicate pair\n",
    "                            #print((i,j))\n",
    "                            unique_pairs.add((i,j))\n",
    "    return unique_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many slices would give a lot of false positives while too few slices would only be able to identify the highest degrees of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands = 4\n",
    "rows = 25\n",
    "max_bucket_item = 35\n",
    "unique_pairs = LSH_bucket(sig_matrix,max_bucket_item)\n",
    "len(unique_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78373"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_pairs = set()\n",
    "for pair in unique_pairs:\n",
    "    sigi = sig_matrix[:,pair[0]].ravel().tolist()[0]\n",
    "    sigj = sig_matrix[:,pair[1]].ravel().tolist()[0]\n",
    "    jsim = calculate_jsim(sigi, sigj)\n",
    "    if jsim > 0.5:\n",
    "        sim_pairs.add(pair)\n",
    "len(sim_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pairs = []\n",
    "for pair in sim_pairs:\n",
    "    jsim = calculate_jsim(m_by_u[pair[0]], m_by_u[pair[1]])\n",
    "    if jsim>0.5:\n",
    "        true_pairs.append(pair)\n",
    "len(true_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time is 12.65 minutes\n"
     ]
    }
   ],
   "source": [
    "cal_time = round(time.time()-t0)\n",
    "print(\"Total running time is {0:.2f} minutes\".format(cal_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbucket_list[x].add(index[0])\\nsig_matrix[:,6487].tolist()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "bucket_list[x].add(index[0])\n",
    "sig_matrix[:,6487].tolist()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
