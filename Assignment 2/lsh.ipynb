{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "About 100.000 users that watched in total 17.770 movies;\n",
    "- Each user watched between 300 and 3000 movies\n",
    "- The file contains about 65.000.000 records (720 MB) of the form:\n",
    "`<user_id, movie_id> : “user_id watched movie_id”`\n",
    "- Similarity between users: Jaccard similarity of sets of movies they watched: \n",
    "``` \n",
    "jsim(S1, S2) = #intersect(S1, S2)/#union(S1, S2)\n",
    "``` \n",
    "- Task: find (with help of LSH) pairs of users whose jsim > 0.5\n",
    "\n",
    "Process:\n",
    "1. Tune it (signature length, number of bands, number of rows per band)\n",
    "2. Randomize, optimize, benchmark, polish the code, ...\n",
    "3. Dump results to a text file ans.txt (just a csv list of records: user1, user2) \n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "t0 = time.time()\n",
    "np.random.seed(seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  load the data\n",
    "FILE = '../data/user_movie.npy'\n",
    "df = pd.DataFrame(np.load(FILE), columns = ['user','movie'])\n",
    "\n",
    "m_by_u = df.groupby('user')['movie'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages and data importing takes 0.55 minutes\n"
     ]
    }
   ],
   "source": [
    "import_time = round(time.time()-t0)\n",
    "print(\"Packages and data importing takes {0:.2f} minutes\".format(import_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinHash\n",
    "\n",
    "The dataset is so small that random permutations is used instead of hash functions. \n",
    "In this way we can avoid time consuming loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we have compared the computing time for two minhashing function.\n",
    "1. use csc_matrix with two loops (signature then user)\n",
    "2. use one loop for users, calculating all sigatures for one user at the same time\n",
    "\n",
    "**The second method takes about half of them of the 1st one. Therefore, we adopt the second method for minhashing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relatively short signatures (50-150) should result in good results (and take less time to compute).\n",
    "n_sig = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1, discarded due to slowness\n"
     ]
    }
   ],
   "source": [
    "print('Method 1, discarded due to slowness')\n",
    "\n",
    "# set user as the column, movie as the row in the sparse matrix\n",
    "#mat = csc_matrix(([1]*df.shape[0], (df.iloc[:,1], df.iloc[:,0])))\n",
    "\n",
    "#n_movie = mat.shape[0]\n",
    "#n_user = mat.shape[1]\n",
    "\n",
    "def minhashing(mat, n_sig = n_sig):\n",
    "    # create the sig matrix using minhashing\n",
    "    sig_matrix = np.zeros([n_sig, n_user])\n",
    "    for i in range(n_sig):\n",
    "        perm = mat[np.random.permutation(n_movie)]\n",
    "        for j in range(n_user):\n",
    "            sig_matrix[i,j] = int(perm.indices[perm.indptr[j]:perm.indptr[j+1]].min())\n",
    "        if i%10==9: # check the progress\n",
    "            print(n_sig - i - 1,' signatures remaining to be created')\n",
    "    return sig_matrix\n",
    "#sig_matrix = minhashing(mat,n_sig)\n",
    "\n",
    "#cal_time = round(time.time()-import_time)\n",
    "#print(\"Minhashing takes {0:.2f} minutes\".format(cal_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103702  users remaining to be created\n",
      "93702  users remaining to be created\n",
      "83702  users remaining to be created\n",
      "73702  users remaining to be created\n",
      "63702  users remaining to be created\n",
      "53702  users remaining to be created\n",
      "43702  users remaining to be created\n",
      "33702  users remaining to be created\n",
      "23702  users remaining to be created\n",
      "13702  users remaining to be created\n",
      "3702  users remaining to be created\n"
     ]
    }
   ],
   "source": [
    "n_movie = df.movie.max()+1\n",
    "n_user = df.user.max()+1\n",
    "def minhashing2(m_by_u, n_sig = n_sig):\n",
    "    perm_matrix = np.array([np.random.permutation(n_movie) for i in range(n_sig)])\n",
    "    sig_matrix = np.zeros([n_sig, n_user])\n",
    "    for i,item in m_by_u.items():\n",
    "        sig =  perm_matrix[:,item].min(axis=1)\n",
    "        sig_matrix[:,i] = sig\n",
    "        if i%10000==0: # check the progress\n",
    "            print(n_user - i - 1,' users remaining to be created')\n",
    "    return sig_matrix\n",
    "sig_matrix = minhashing2(m_by_u,n_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minhashing completed, taking 2.85 minutes\n"
     ]
    }
   ],
   "source": [
    "cal_time = round(time.time()-t0)\n",
    "print(\"Minhashing completed, taking {0:.2f} minutes\".format(cal_time/60 - import_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.savetxt('sig.txt', sig_matrix, fmt='%d')\n",
    "#sig_matrix = np.loadtxt('sig.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the LSH algorithm\n",
    "\n",
    "When there are many users that fall into the same bucket (i.e., there are many candidates for being similar to each other) then checking if all the potential pairs are really similar might be very expensive: you have to check k(k-1)/2 pairs, when the bucket has k elements. Postpone evaluation of such a bucket till the very end (or just ignore it – they are really expensive). Or better: consider increasing the number of rows per band – that will reduce the chance of encountering big buckets.\n",
    "\n",
    "Note that b*r doesn’t have to be exactly the length of the signature. For example, when you work with signature of length n=100, you may consider e.g., b=3, r=33, b=6, r=15, etc. – any combination of b\n",
    " \n",
    "To make sure that your program will not exceed the 30 minutes runtime you are advised to close the result.txt file after any new pair is appended to it (and open it again, when you want to append a new one).\n",
    "\n",
    "### jsim functions for signatures matrix and movies by user list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_estimate_pair(pair,n_sig):\n",
    "    # check if the pair from the bucket is similar based on sig matrix\n",
    "    sigi = sig_matrix[:,pair[0]]#.tolist()#.ravel().tolist()[0]\n",
    "    sigj = sig_matrix[:,pair[1]]#.tolist()#.ravel().tolist()[0]\n",
    "    # estimate_jsim\n",
    "    intersect = np.count_nonzero(sigi == sigj)\n",
    "    jsim = intersect/n_sig\n",
    "    if jsim > 0.40:\n",
    "        return True\n",
    "\n",
    "def check_true_pair(pair):\n",
    "    # check if the similar pair based on sig matrix is a true pair\n",
    "    a = m_by_u[pair[0]]\n",
    "    b = m_by_u[pair[1]]\n",
    "    intersect = len(set(a) & set(b))\n",
    "    union = len(a) + len(b) - intersect\n",
    "    jsim = intersect/union\n",
    "    if jsim > 0.5:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing bands, rows and the bucket selection creteria\n",
    "Too many slices would give a lot of false positives while too few slices would only be able to identify the highest degrees of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bands = 30\n",
    "rows = 5\n",
    "max_bucket_item = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|                                   | 1        | 2 |   |\n",
    "|-----------------------------------|----------|---|---|\n",
    "| bands                             | 30       | 25 |   |\n",
    "| rows                              | 5        |6   |   |\n",
    "| max_bucket_item                   | 300      | 300  |   |\n",
    "| Unique pairs                      | 21482770 |  7547562 |   |\n",
    "| Similar pairs based on sig matrix | 878065   |  331523 |   |\n",
    "| True similar pairs                | 639      |  326 |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have compared the results for two hashing functions.\n",
    "1. use sum of the band as the hashing function\n",
    "2. use the tuple of numbers and default hash() \n",
    "\n",
    "**The second method dramatically decrease the number of false postive items. We adopt the second method for our assignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSH_bucket(sig_matrix, max_bucket_item = 25):\n",
    "    unique_pairs = set() \n",
    "    for b in range(bands):\n",
    "        bucket_list = defaultdict(set) # remove the duplicate items\n",
    "        s = np.sum(sig_matrix[b*rows:(b+1)*rows,:], axis =0)\n",
    "        for index, x in np.ndenumerate(s):\n",
    "            bucket_list[x].add(index[0])\n",
    "        for key, value in bucket_list.items():\n",
    "            if len(value) > 1 and len(value) < max_bucket_item: \n",
    "                for i in value:\n",
    "                    for j in value:\n",
    "                        if i < j and ((i,j) not in unique_pairs): # remove the duplicate pair\n",
    "                            unique_pairs.add((i,j))\n",
    "    return unique_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSH_bucket_2(sig_matrix, max_bucket_item = 25):\n",
    "    unique_pairs = set() \n",
    "    for b in range(bands):\n",
    "        bucket_list = defaultdict(set) # remove the duplicate items\n",
    "        df = pd.DataFrame(sig_matrix[b*rows:(b+1)*rows,:])\n",
    "        h = df.apply(lambda x: hash(tuple(x)), axis = 0)\n",
    "        for index, x in np.ndenumerate(h):\n",
    "            bucket_list[x].add(index[0])\n",
    "        for key, value in bucket_list.items():\n",
    "            if len(value) > 1 and len(value) < max_bucket_item: \n",
    "                for i in value:\n",
    "                    for j in value:\n",
    "                        if i < j and ((i,j) not in unique_pairs): # remove the duplicate pair\n",
    "                            unique_pairs.add((i,j))\n",
    "    return unique_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check, no write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pairs:  7547562\n",
      "Similar pairs based on sig matrix:  331523\n",
      "True similar pairs:  326\n"
     ]
    }
   ],
   "source": [
    "def print_not_write():\n",
    "    unique_pairs = LSH_bucket_2(sig_matrix,max_bucket_item)\n",
    "    print('Unique pairs: ',len(unique_pairs))\n",
    "\n",
    "    sim_pairs = []\n",
    "    for pair in unique_pairs:\n",
    "        if check_estimate_pair(pair,n_sig):\n",
    "            sim_pairs.append(pair)\n",
    "    print('Similar pairs based on sig matrix: ',len(sim_pairs))\n",
    "\n",
    "    true_pairs = []\n",
    "    for pair in sim_pairs:\n",
    "        if check_true_pair(pair):\n",
    "            true_pairs.append(pair) \n",
    "    print('True similar pairs: ', len(true_pairs))\n",
    "print_not_write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to text file, for final submital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSH_bucket_true_paris(sig_matrix, max_bucket_item = 25): \n",
    "    unique_pairs = set() \n",
    "    for b in range(bands):\n",
    "        bucket_list = defaultdict(set) # remove the duplicate items\n",
    "        df = pd.DataFrame(sig_matrix[b*rows:(b+1)*rows,:])\n",
    "        h = df.apply(lambda x: hash(tuple(x)), axis = 0)\n",
    "        for index, x in np.ndenumerate(h):\n",
    "            bucket_list[x].add(index[0])\n",
    "        for key, value in bucket_list.items():\n",
    "            if len(value) > 1 and len(value) < max_bucket_item: \n",
    "                for i in value:\n",
    "                    for j in value:\n",
    "                        if i < j and ((i,j) not in unique_pairs): # remove the duplicate pair\n",
    "                            unique_pairs.add((i,j))\n",
    "                            if check_estimate_pair((i,j),n_sig):\n",
    "                                if check_true_pair((i,j)):\n",
    "                                    with open('ans.txt','a') as file:\n",
    "                                        writer = csv.DictWriter(file,fieldnames=['user1', 'user2'])\n",
    "                                        writer.writerow({'user1':i,'user2':j})\n",
    "LSH_bucket_true_paris(sig_matrix,max_bucket_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time is 21.13 minutes\n"
     ]
    }
   ],
   "source": [
    "cal_time = round(time.time()-t0)\n",
    "print(\"Total running time is {0:.2f} minutes\".format(cal_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- [hash tuple](https://stackoverflow.com/questions/25757042/create-hash-value-for-each-row-of-data-with-selected-columns-in-dataframe-in-pyt/25757564)\n",
    "- [hash string](https://www.pythoncentral.io/hashing-strings-with-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
